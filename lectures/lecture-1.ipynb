{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scroll': 'true',\n",
       " 'start_slideshow_at': 'selected',\n",
       " 'theme': 'sky',\n",
       " 'transition': 'zoom'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "cm = BaseJSONConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "            'scroll': 'true'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Floating-point arithmetic, vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** floating point, vector norms, matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today \n",
    "- Fixed/floating point arithmetic;\n",
    "- Concept of **backward** and **forward** stability of algorithms\n",
    "- How to measure accuracy: vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of numbers\n",
    "\n",
    "Real numbers represent quantities: probabilities, velocities, masses, ...\n",
    "\n",
    "It is important to know, how they are represented in the computer (which only knows about bits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed point\n",
    "\n",
    "The most straightforward format for the representation of real numbers is **fixed point** representation,\n",
    "\n",
    "also known as **Qm.n** format.\n",
    "\n",
    "A **Qm.n** number is in the range $[-(2^m), 2^m - 2^{-n}]$, with resolution $2^{-n}$.\n",
    "\n",
    "Total storage is $m + n + 1$ bits.\n",
    "\n",
    "The range of numbers represented is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of numbers\n",
    "The numbers in computer memory are typically represented as **floating point numbers** \n",
    "\n",
    "A floating point number is represented as  \n",
    "\n",
    "$$\\textrm{number} = \\textrm{significand} \\times \\textrm{base}^{\\textrm{exponent}},$$\n",
    "\n",
    "where *significand* is integer, *base* is positive integer  and *exponent* is integer (can be negative), i.e.\n",
    "\n",
    "$$ 1.2 = 12 \\cdot 10^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed vs Floating\n",
    "\n",
    "**Q**: What are the advantages/disadvantages of the fixed and floating points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A**:  In most cases, they work just fine.\n",
    "\n",
    "However, fixed point represents numbers within specified range and controls **absolute** accuracy.\n",
    "\n",
    "Floating point represent numbers with **relative** accuracy, and is suitable for the case when numbers in the computations have varying scale \n",
    "\n",
    "(i.e., $10^{-1}$ and $10^{5}$).\n",
    "\n",
    "In practice, if speed is of no concern, use float32 or float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IEEE 754\n",
    "In modern computers, the floating point representation is controlled by [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point) which was published in **1985** and before that point different computers behaved differently with floating point numbers. \n",
    "\n",
    "IEEE 754 has:\n",
    "- Floating point representation (as described above), $(-1)^s \\times c \\times b^q$.\n",
    "- Two infinities, $+\\infty$ and $-\\infty$\n",
    "- Two kinds of **NaN**: a quiet NaN (**qNaN**) and signalling NaN (**sNaN**) \n",
    "- Rules for **rounding**\n",
    "- Rules for $\\frac{0}{0}, \\frac{1}{-0}, \\ldots$\n",
    "\n",
    "$ 0 \\leq c \\leq b^p - 1, \\quad 1 - emax \\leq q + p - 1 \\leq emax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The two most common format, single & double\n",
    "\n",
    "The two most common format, called **binary32** and **binary64** (called also **single** and **double** formats).\n",
    "\n",
    "| Name | Common Name | Base | Digits | Emin | Emax |\n",
    "|------|----------|----------|-------|------|------|\n",
    "|binary32| single precision | 2 | 11 | -14 | + 15 |  \n",
    "|binary64| double precision | 2 | 24 | -126 | + 127 |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy and memory\n",
    "The **relative accuracy** of single precision is $10^{-7}-10^{-8}$,  \n",
    "\n",
    "while for double precision is $10^{-14}-10^{-16}$.\n",
    "\n",
    "<font color='red'> Crucial note 1: </font> A **float32** takes **4 bytes**, **float64**, or double precision, takes **8 bytes.**\n",
    "\n",
    "<font color='red'> Crucial note 2: </font> These are the only two floating point-types supported in hardware.\n",
    "\n",
    "<font color='red'> Crucial note 3: </font> You should use **double precision** in CSE and **float** on GPU/Data Science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some demo (for division accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1040364727377892\n",
      "-5.96046e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#c = random.random()\n",
    "#print(c)\n",
    "c = np.float32(0.925924589693)\n",
    "a = np.float32(8.9)\n",
    "b = np.float32(c / a)\n",
    "print('{0:10.16f}'.format(b))\n",
    "print(a * b - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000001468220603\n"
     ]
    }
   ],
   "source": [
    "#a = np.array(1.585858585887575775757575e-5, dtype=np.float)\n",
    "a = np.array(5.0, dtype=np.float32)\n",
    "b = np.sqrt(a)\n",
    "print('{0:10.16f}'.format(b ** 2 - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array(2.28827272710, dtype=np.float32)\n",
    "b = np.exp(a)\n",
    "print(np.log(b) - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- For some values the inverse functions give exact answers\n",
    "- The relative accuracy should be kept due to the IEEE standard.\n",
    "- Does not hold for many modern GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss of significance\n",
    "\n",
    "Many operations lead to the loss of digits [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance)\n",
    "\n",
    "\n",
    "For example, **it is a bad idea to subtract two big numbers that are close, the difference will have fewer correct digits**.\n",
    "\n",
    "This is related to algorithms and their properties (forward/backward stability), which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summation algorithm\n",
    "\n",
    "However, the rounding errors can depend on the algorithm. \n",
    "\n",
    "Consider the simplest problem: given $n$ floating point numbers $x_1, \\ldots, x_n$  \n",
    "\n",
    "compute their sum\n",
    "\n",
    "$$S = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.$$\n",
    "\n",
    "The simplest algorithm is to add one-by-one. \n",
    "\n",
    "What is the actual error for such algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive algorithm\n",
    "\n",
    "Naive algorithm adds numbers one-by-one, \n",
    "\n",
    "$$y_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.$$\n",
    "\n",
    "The worst-case error is then proportional to $\\mathcal{O}(n)$, while **mean-squared** error is $\\mathcal{O}(\\sqrt{n})$. <font color='red'>?? </font>\n",
    "\n",
    "The **Kahan algorithm** gives the worst-case error bound $\\mathcal{O}(1)$ (i.e., independent of $n$).  \n",
    "\n",
    "<font color='red'> Can you find the $\\mathcal{O}(\\log n)$ algorithm? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kahan summation\n",
    "The following algorithm gives $2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2)$ error, where $\\varepsilon$ is the machine precision.\n",
    "```python\n",
    "s = 0\n",
    "c = 0\n",
    "for i in range(len(x)):\n",
    "    y = x[i] - c\n",
    "    t = s + y\n",
    "    c = (t - s) - y\n",
    "    s = t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in numpy sum: 1.86e-04, \n",
      "kahan: -1.3e-07, \n",
      "dumb_sum: -1.0e-02. \n",
      "math_sum: 1.3e-10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "n = 10 ** 8\n",
    "sm = 1e-10\n",
    "x = np.ones(n, dtype=np.float32) * sm\n",
    "x[0] = 1.0\n",
    "true_sum = 1.0 + (n - 1)*sm\n",
    "approx_sum = np.sum(x)  \n",
    "math_sum = math.fsum(x) ## so use math sum RATHER THAN numpy??\n",
    "\n",
    "from numba import jit\n",
    "@jit\n",
    "def dumb_sum2(x):\n",
    "    s = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        s = s + x[i]\n",
    "    return s\n",
    "@jit\n",
    "def kahan_sum(x):\n",
    "    s = np.float32(0.0)\n",
    "    c = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "    return s\n",
    "k_sum = kahan_sum(x)\n",
    "d_sum = dumb_sum2(x)\n",
    "print('Error in numpy sum: {0:2.2e}, \\nkahan: {1:3.1e}, \\ndumb_sum: {2:3.1e}. \\nmath_sum: {3:3.1e}'.format(\\\n",
    "                                approx_sum - true_sum, k_sum - true_sum, d_sum - true_sum, math_sum - true_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(math.fsum([1, 1e20, 1, -1e20]), np.sum([1, 1e20, 1, -1e20]), 1 + 1e20 + 1 - 1e20)  ## numpy sum is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of floating-point \n",
    "You should be really careful with floating point, since it may give you incorrect answers due to rounding-off errors.\n",
    "\n",
    "For many standard algorithms, the stability is well-understood and problems can be easily detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "In NLA we typically work not with **numbers**, but with **vectors**. \n",
    "\n",
    "Recall that a vector in a fixed basis of size $n$ can be represented as a 1D array with $n$ numbers. \n",
    "\n",
    "Typically, it is considered as an $n \\times 1$ matrix (**column vector**).\n",
    "\n",
    "**Example:** \n",
    "Polynomials with degree $\\leq n$ form a linear space. \n",
    "Polynomial $ x^3 - 2x^2 + 1$ can be considered as a vector $\\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix}$ in the basis $\\{x^3, x^2, x, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector norm\n",
    "Vectors typically provide an (approximate) description of a physical (or some other) object. \n",
    "\n",
    "One of the main question is **how accurate** the approximation is (1%, 10%). \n",
    "\n",
    "What is an acceptable representation, of course, depends on the particular applications. For example:\n",
    "- In partial differential equations accuracies $10^{-5} - 10^{-10}$ are the typical case\n",
    "- In data mining sometimes an error of $80\\%$ is ok, since the interesting signal is corrupted by a huge noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances and norms\n",
    "Norm is a **qualitative measure of smallness of a vector** and is typically denoted as $\\Vert x \\Vert$.\n",
    "\n",
    "The norm should satisfy certain properties:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$,\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (triangle inequality),\n",
    "- If $\\Vert x \\Vert = 0$ then $x = 0$.\n",
    "\n",
    "The distance between two vectors is then defined as\n",
    "$$\n",
    "   d(x, y) = \\Vert x - y \\Vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard norms\n",
    "The most well-known and widely used norm is **euclidean norm**:\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "which corresponds to the distance in our real life (the vectors might have complex elements, thus is the modulus here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $p$-norm\n",
    "Euclidean norm, or $2$-norm, is a subclass of an important class of $p$-norms:\n",
    "$$\n",
    " \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n",
    "$$\n",
    "There are two very important special cases:\n",
    "- Infinity norm, or Chebyshev norm which is defined as the maximal element: $\\Vert x \\Vert_{\\infty} = \\max_i | x_i|$\n",
    "- $L_1$ norm (or **Manhattan distance**) which is defined as the sum of modules of the elements of $x$: $\\Vert x \\Vert_1 = \\sum_i |x_i|$  \n",
    "<img src=\"pics/chebyshev.jpeg\" style=\"float: left; height: 1%\">  <img src=\"pics/manhattan.jpeg\" style=\"height\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will give examples where Manhattan is very important: it all relates to the **compressed sensing** methods \n",
    "that emerged in the mid-00s as one of the most popular research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equivalence of the norms\n",
    "All norms are equivalent in the sense that\n",
    "$$\n",
    "   C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_*\n",
    "$$  \n",
    "for some constants $C_1(n), C_2(n)$, $x \\in \\mathbb{R}^n$ for any pairs of norms $\\Vert \\cdot \\Vert_*$ and $\\Vert \\cdot \\Vert_{**}$. The equivalence of the norms basically means that <font color='blue'>**if the vector is small in one norm, it is small in another norm**</font>. However, the constants can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing norms in Python\n",
    "The numpy package has all you need for computing norms (```np.linalg.norm``` function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error: 0.00343922107489\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = np.ones(n)\n",
    "b = a + 1e-3 * np.random.randn(n)\n",
    "print('Relative error:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unit disks in different norms\n",
    "A unit disk is a set of point such that $\\Vert x \\Vert \\leq 1$. For the Frobenius norm is a disk; for other norms the \"disks\" look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x127abd208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njIlggcEYY0wECwzGGGMi/P8LBjHNZdLiNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dac5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "p = 2. # Which norm do we use\n",
    "M = 40000 # Number of sampling points\n",
    "a = np.random.randn(M, 2)\n",
    "b = []\n",
    "for i in range(M):\n",
    "    if np.linalg.norm(a[i, :], p) <= 1:\n",
    "        b.append(a[i, :])\n",
    "b = np.array(b)\n",
    "plt.plot(b[:, 0], b[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why $L_1$-norm can be important\n",
    "$L_1$ norm, as it was discovered quite recently, plays an important role in **compressed sensing**. \n",
    "\n",
    "The simplest formulation is as follows:\n",
    "- You have some observations $f$ \n",
    "- You have a linear model $Ax = f$, where $A$ is an $n \\times m$ matrix, $A$ is **known**\n",
    "- The number of equations, $n$ is less than the number of unknowns, $m$\n",
    "\n",
    "The question: can we find the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution is obviously non-unique, so a natural approach is to find the solution that is minimal in the certain sense:\n",
    "\n",
    "$$ \\Vert x \\Vert \\rightarrow \\min, \\quad \\mbox{subject to } Ax = f$$\n",
    "\n",
    "Typical choice of $\\Vert x \\Vert = \\Vert x \\Vert_2$ leads to the **linear least squares problem** (and has been used for ages).  \n",
    "\n",
    "The choice $\\Vert x \\Vert = \\Vert x \\Vert_1$ leads to the [**compressed sensing**]\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Compressed_sensing) and what happens, it typically yields the **sparsest solution**.  \n",
    "\n",
    "[A short demo](tv-denoising-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a stable algorithm?\n",
    "\n",
    "And we finalize the lecture by the concept of **stability**.\n",
    "\n",
    "Let $x$ be an object (for example, a vector). Let $f(x)$ be the function (functional) you want to evaluate.  \n",
    "\n",
    "You also have a **numerical algorithm** ``alg(x)`` that actually computes **approximation** to $f(x)$.  \n",
    "\n",
    "The algorithm is called <font color='blue'>**forward stable**</font>, if $$\\Vert alg(x) - f(x) \\Vert  \\leq \\varepsilon $$  \n",
    "\n",
    "The algorithm is called <font color='blue'>**backward stable**</font>, if for any $x$ there is a close vector $x + \\delta x$ such that\n",
    "\n",
    "$$alg(x) = f(x + \\delta x)$$\n",
    "\n",
    "and $\\Vert \\delta x \\Vert$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical example\n",
    "A classical example is the **solution of linear systems of equations** using LU-factorizations\n",
    "\n",
    "We consider the **Hilbert matrix** with the elements\n",
    "\n",
    "$$A = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.$$\n",
    "\n",
    "And consider a linear system\n",
    "\n",
    "$$Ax = f.$$\n",
    "\n",
    "(We will look into matrices in more details in the next lecture, and for linear systems in the upcoming weeks, but now you actually **see** the linear system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.7752592502\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 500\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] #Hil\n",
    "a = np.array(a)\n",
    "rhs =  np.random.random(n)\n",
    "sol = np.linalg.solve(a, rhs)\n",
    "print(np.linalg.norm(a.dot(sol) - rhs)/np.linalg.norm(rhs)) #Ax - y\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.75686455918e-07\n"
     ]
    }
   ],
   "source": [
    "rhs =  np.ones(n)\n",
    "sol = np.linalg.solve(a, rhs)\n",
    "print(np.linalg.norm(a.dot(sol) - rhs)/np.linalg.norm(rhs)) #Ax - y\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Floating point  (double, single, number of bytes), rounding error\n",
    "- Norms are measures of smallness, used to compute the accuracy\n",
    "- $1$, $p$ and Euclidean norms \n",
    "- $L_1$ is used in compressed sensing as a surrogate for sparsity (later lectures) \n",
    "- Forward/backward error (and stability of algorithms)  (later lectures)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
